{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNlswp6ofV1niiUaoY4/4Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiseAboveAll/PYTORCH_Learning/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-9hzjFz3Ybn",
        "colab_type": "text"
      },
      "source": [
        "# Sequential Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDK0Xyi13esl",
        "colab_type": "text"
      },
      "source": [
        "- Example of sequential data : Text , Speech, Financial Data\n",
        "\n",
        "<h1> Types Of Sequence Data : </h1>\n",
        "\n",
        "1. Time Series Data\n",
        "\n",
        "  - Any continues value measurement recorded periodically \n",
        "\n",
        "2. Text \n",
        "\n",
        "  - Can be treated as sequence \n",
        "\n",
        "  - In ML we used to create Bag of Words, Using this we used to loose information.\n",
        "\n",
        "<h1> What is Sequence ? </h1>\n",
        "\n",
        "Length of data is measured by time , represented by letter 'T'.\n",
        "\n",
        "Shape of Data = N x T x D\n",
        "\n",
        "N -> N.O of Samples\n",
        "\n",
        "T -> Time Step\n",
        "\n",
        "D -> N.O of Features\n",
        "\n",
        "Most of the time we have variable length sequences, i.e if T is variable i.e not constant length of sequence in data will differ but we require constant length of all . Example sentences , words in sentences differ. We can pad the sequences with 0s so that all the sequence at T(n) become constant length. \n",
        "\n",
        "In pytorch , instead of having all the sequences padded to the same length as the longest sequence, just do this accross single batches. Pytorch try to group sequences on the basis of similarity in length , hence batches will be random but not as random as it would be.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpAjwWlYcyec",
        "colab_type": "text"
      },
      "source": [
        "# Forecasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_kH9L0dj_44",
        "colab_type": "text"
      },
      "source": [
        "- We have time series , we have to predict next values of the time series.\n",
        "\n",
        "- Number of future steps we want to predict is called Horizon\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URO3hcr8md3h",
        "colab_type": "text"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrMzFq_vmgBb",
        "colab_type": "text"
      },
      "source": [
        "**In ANN we used to calculate the output of hidden node using the input vector and output of output node using the output of hidden node as input . X -> H -> Y_hat.**\n",
        "\n",
        "**H = a(W.T . X + b)**\n",
        "\n",
        "**Y_hat = a(W.T . H + b)**\n",
        "\n",
        "Whereas,\n",
        "\n",
        "**In RNN, the *Hidden Node*  loop back to itself, i.e in other words the *Hidden Node* does not only depends on the input, but also on it's own previous (time-stamp) value. This *Hidden Node is also called *Hidden State*. This Hidden State is non linear function of past values. This loop-back to previous value states that there is time delay of one step**\n",
        "\n",
        "<h3> RNN Equation </h3>\n",
        "\n",
        "Ht = a(Wxh.T . X + Whh.T . Ht-1 + b)\n",
        "\n",
        "Y_hat = a(W.T . Ht + b)\n",
        "\n",
        "<h3> Calculation </h3>\n",
        "\n",
        "Given : x1,x2,x3,....,xt\n",
        "\n",
        "Shape(Xt) : D\n",
        "\n",
        "First Step : \n",
        "\n",
        "-> h1 = a(Wxh.T . x1 + Whh.T . h0 + b)\n",
        "\n",
        "-> y_hat1 = a(W.T . h1 + b)\n",
        "\n",
        "Now we have x2 & h1 :\n",
        "\n",
        "-> h2 = a(Wxh.T . x2 + Whh.T . h1 + b)\n",
        "\n",
        "-> y_hat2 = a(W.T . h2 + b)\n",
        "\n",
        "We can repeat this process till we calculate hT and y_hatT. \n",
        "\n",
        "**Each y_hatT depends on x1,x2,x3,...,xt, we get yhat for each corresponding x1,x2,x3,....,xt...xT. xT gives us the final prediction , i.e final y_hat , hence all the previous time-stamps y_hat will be removed. There are instances when we do not discard the previous y_hats , instances like neural machine translation. In machine translation , both input and output are senteces but in different languages. Both the input and output are sequences, capture predctions at each time point. \n",
        "\n",
        "<h3>Classification Probability</h3>\n",
        "\n",
        "- In ANN and CNN after applying softmax the output prediction is probability distribution. P(y=k|X)\n",
        "\n",
        "- For neural machine translation, it is also a classification, but we will have probability of yt=k given something. P(y=k|?) . \n",
        "\n",
        "What is this 'Something'?\n",
        "\n",
        "![RNN](https://raw.githubusercontent.com/RiseAboveAll/PYTORCH_Learning/master/RNN.PNG)\n",
        "\n",
        "We can say that y_hatT depends on every X in the input sequence. Hence:\n",
        "\n",
        "P(y1=k|x1)\n",
        "\n",
        "P(y2=k|X2,X1)\n",
        "\n",
        "P(y3=k|X3,X2,X1)\n",
        "\n",
        ".\n",
        ".\n",
        ".\n",
        ".\n",
        "\n",
        "P(yT=k|XT,...,X3,X2,X1)\n",
        "\n",
        "RNN accounts for all the previous words in the sentence not only the previous words. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlPb_qC_Ud7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhKv0oSH3SDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}